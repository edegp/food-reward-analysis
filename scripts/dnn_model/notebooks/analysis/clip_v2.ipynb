{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIPでの主観的価値の推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform, pearsonr\n",
    "\n",
    "from src.const import DATA_PATH\n",
    "\n",
    "plt.rcParams[\"font.serif\"] = [\"noto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = pd.read_csv(os.path.join(DATA_PATH, \"data_responses_NCNP_2types.csv\"))\n",
    "food_value = pd.read_csv(os.path.join(DATA_PATH, \"food_value.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp[\"is_obesity\"] = resp[\"BMI\"] >= 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "被験者 [ 50  83 104 121 130 137 138 143 147 150] 10 人を除外\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "img  is_obesity\n",
       "1    False         5.945455\n",
       "     True          5.853933\n",
       "2    False         6.190909\n",
       "     True          6.438202\n",
       "3    False         6.118182\n",
       "                     ...   \n",
       "894  True          4.460674\n",
       "895  False         4.209091\n",
       "     True          3.258427\n",
       "896  False         5.581818\n",
       "     True          4.752809\n",
       "Name: res_L, Length: 1792, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier = resp[\"sub_ID\"].unique()[\n",
    "        (resp.groupby(\"sub_ID\")[\"res_L\"].value_counts().unstack() > 896 * 0.75).any(\n",
    "            axis=1\n",
    "        )\n",
    "        | (\n",
    "            (resp.groupby(\"sub_ID\")[\"res_L\"].unique().apply(lambda x: len(x)) <= 4)\n",
    "            & (\n",
    "                resp.groupby(\"sub_ID\")[\"res_L\"].value_counts().unstack() > 896 * 0.65\n",
    "            ).any(axis=1)\n",
    "        )\n",
    "    ]\n",
    "print(\"被験者\", outlier, len(outlier), \"人を除外\")\n",
    "res_L_mean = (\n",
    "    resp.groupby([\"img\", \"is_obesity\"])[\"res_L\"].mean()\n",
    ")\n",
    "res_H_mean = (\n",
    "    resp.groupby([\"img\", \"is_obesity\"])[\"res_H\"].mean()\n",
    ")\n",
    "res_T_mean = (\n",
    "    resp.groupby([\"img\", \"is_obesity\"])[\"res_T\"].mean()\n",
    ")\n",
    "res_L_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.const import ROOT_PATH\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"image_path\": [\n",
    "            os.path.join(ROOT_PATH, \"Database\", f\"{str(img).zfill(4)}.jpg\")\n",
    "            for img in resp[\"img\"]\n",
    "        ],\n",
    "        \"caption\": [\n",
    "            f\"This food is {food_value.loc[food_value[\"id\"] == img, \"Item_description\"].values[0]}\"\n",
    "            for img in resp[\"img\"]\n",
    "        ],\n",
    "        \"res_L\": resp[\"res_L\"],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# \"\"\"\n",
    "# Full training pipeline for CLIP joint regression with:\n",
    "#   • TorchVision v2 GPU-based augmentation\n",
    "#   • Multi-view (×5) online data expansion\n",
    "#   • K-fold CV (6 splits)\n",
    "#   • LoRA adapters injected into every attention block (qkv + proj)\n",
    "\n",
    "# Tested on RTX/Ada6000 (48 GB) with bf16.\n",
    "# \"\"\"\n",
    "\n",
    "# # ───────────────────────── 0.  Imports & global config ─────────────────────────\n",
    "# import os\n",
    "# from copy import deepcopy\n",
    "# from pathlib import Path\n",
    "# from typing import List, Set, Sequence\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from PIL import Image\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset\n",
    "# from torchvision import transforms\n",
    "# import torchvision.transforms.v2 as T\n",
    "# from sklearn.model_selection import KFold\n",
    "# from scipy.stats import pearsonr\n",
    "\n",
    "# from transformers import Trainer, TrainingArguments\n",
    "# from transformers.utils import logging as hf_logging\n",
    "# from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# from open_clip import create_model_and_transforms, get_tokenizer\n",
    "\n",
    "\n",
    "# hf_logging.set_verbosity_info()\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# CLIP_NAME = \"hf-hub:timm/resnet50x4_clip.openai\"  # ResNet-50×4 backbone\n",
    "\n",
    "# # ───────────────────────── 1.  Load CLIP preprocess & tokenizer ───────────────\n",
    "# clip_backbone, _, clip_preprocess = create_model_and_transforms(CLIP_NAME)\n",
    "# _tokenizer = get_tokenizer(CLIP_NAME)\n",
    "\n",
    "\n",
    "# def tokenize(text: str):\n",
    "#     \"\"\"OpenCLIP tokenizer returns (1,77); squeeze to (77,) → torch.long\"\"\"\n",
    "#     return _tokenizer(text).squeeze(0).to(torch.long)\n",
    "\n",
    "\n",
    "# # grab mean/std from clip_preprocess.Normalise\n",
    "# norm_layer = next(\n",
    "#     t for t in clip_preprocess.transforms if isinstance(t, transforms.Normalize)\n",
    "# )\n",
    "# CLIP_MEAN, CLIP_STD = norm_layer.mean, norm_layer.std\n",
    "\n",
    "# # ───────────────────────── 2. TorchVision-v2 augment pipeline ─────────────────\n",
    "\n",
    "\n",
    "# def build_train_transform(device: str = \"cpu\"):\n",
    "#     return T.Compose(\n",
    "#         [\n",
    "#             T.ToImage(),\n",
    "#             T.Resize((288, 288)),\n",
    "#             T.CenterCrop(288),\n",
    "#             T.RandomHorizontalFlip(),\n",
    "#             T.RandomAffine(\n",
    "#                 degrees=20, translate=(0.2, 0.2), scale=(0.7, 1.2), fill=(255, 255, 255)\n",
    "#             ),\n",
    "#             T.GaussianBlur(kernel_size=5, sigma=(0.01, 4.0)),\n",
    "#             T.ColorJitter(brightness=0.075, contrast=0.0, saturation=0.03, hue=0.03),\n",
    "#             T.ToDtype(torch.float32, scale=True),\n",
    "#             T.Normalize(CLIP_MEAN, CLIP_STD),\n",
    "#         ]\n",
    "#     ).to(device)\n",
    "\n",
    "\n",
    "# train_transform = build_train_transform(\"cuda\")  # GPU augment\n",
    "# val_transform = clip_preprocess  # original PIL pipeline\n",
    "\n",
    "\n",
    "# # ───────────────────────── 3.  Dataset classes ────────────────────────────────\n",
    "# class ClipCnnDataset(Dataset):\n",
    "#     def __init__(self, df: pd.DataFrame, transform=None):\n",
    "#         self.df = df.reset_index(drop=True)\n",
    "#         self.transform = transform or clip_preprocess\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "#         img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "#         img = self.transform(img)\n",
    "#         return (\n",
    "#             img,\n",
    "#             tokenize(row[\"caption\"]),\n",
    "#             torch.tensor(row[\"res_L\"], dtype=torch.float32),\n",
    "#         )\n",
    "\n",
    "\n",
    "# class MultiViewDataset(Dataset):\n",
    "#     \"\"\"N-view online expansion (orig + aug×4 =5).\"\"\"\n",
    "\n",
    "#     def __init__(self, df: pd.DataFrame, transforms_list: Sequence):\n",
    "#         self.df = df.reset_index(drop=True)\n",
    "#         self.transforms = transforms_list\n",
    "#         self.n_views = len(transforms_list)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df) * self.n_views\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_idx, view_idx = divmod(idx, self.n_views)\n",
    "#         row = self.df.iloc[img_idx]\n",
    "#         img = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "#         img = self.transforms[view_idx](img)\n",
    "#         return (\n",
    "#             img,\n",
    "#             tokenize(row[\"caption\"]),\n",
    "#             torch.tensor(row[\"res_L\"], dtype=torch.float32),\n",
    "#         )\n",
    "\n",
    "\n",
    "# # ───────────────────────── 4.  Injected MHA (QKV split) ───────────────────────\n",
    "# class InjectedMultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, dropout=0.0, bias=True, batch_first=False):\n",
    "#         super().__init__()\n",
    "#         self.embed_dim, self.num_heads, self.dropout, self.batch_first = (\n",
    "#             embed_dim,\n",
    "#             num_heads,\n",
    "#             dropout,\n",
    "#             batch_first,\n",
    "#         )\n",
    "#         self.head_dim = embed_dim // num_heads\n",
    "#         assert self.head_dim * num_heads == embed_dim\n",
    "#         self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=bias)\n",
    "#         self.proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "#         self.sdp = F.scaled_dot_product_attention\n",
    "\n",
    "#     def set_parameters(self, src: nn.MultiheadAttention):\n",
    "#         self.qkv.weight.data.copy_(src.in_proj_weight.data)\n",
    "#         self.qkv.bias.data.copy_(src.in_proj_bias.data)\n",
    "#         self.proj.load_state_dict(src.out_proj.state_dict())\n",
    "\n",
    "#     def forward(\n",
    "#         self, q, k, v, attn_mask=None, key_padding_mask=None, is_causal=False, **kw\n",
    "#     ):\n",
    "#         if self.batch_first and q.dim() == 3:\n",
    "#             q = q.transpose(0, 1)\n",
    "#         L, N, _ = q.shape\n",
    "#         qkv = (\n",
    "#             self.qkv(q)\n",
    "#             .view(L, N, 3, self.num_heads, self.head_dim)\n",
    "#             .permute(2, 1, 3, 0, 4)\n",
    "#         )  # 3,N,h,L,d\n",
    "#         qh, kh, vh = [\n",
    "#             t.contiguous().view(N * self.num_heads, L, self.head_dim) for t in qkv\n",
    "#         ]\n",
    "#         out = self.sdp(\n",
    "#             qh,\n",
    "#             kh,\n",
    "#             vh,\n",
    "#             attn_mask=attn_mask,\n",
    "#             dropout_p=self.dropout if self.training else 0.0,\n",
    "#             is_causal=is_causal,\n",
    "#         )\n",
    "#         out = (\n",
    "#             out.view(N, self.num_heads, L, self.head_dim)\n",
    "#             .permute(2, 0, 1, 3)\n",
    "#             .reshape(L, N, self.embed_dim)\n",
    "#         )\n",
    "#         out = self.proj(out)\n",
    "#         if self.batch_first:\n",
    "#             out = out.transpose(0, 1)\n",
    "#         return out, None\n",
    "\n",
    "\n",
    "# def inject_linear_attention(model: nn.Module, encoders: Set[str] = {\"transformer\"}):\n",
    "#     \"\"\"Replace nn.MultiheadAttention with Injected version (LoRA-friendly).\"\"\"\n",
    "#     for enc in encoders:\n",
    "#         tgt = getattr(model, enc, None)\n",
    "#         if tgt is None or not hasattr(tgt, \"resblocks\"):\n",
    "#             continue\n",
    "#         for blk in tgt.resblocks:\n",
    "#             if isinstance(blk.attn, nn.MultiheadAttention):\n",
    "#                 inj = InjectedMultiHeadAttention(\n",
    "#                     blk.attn.embed_dim,\n",
    "#                     blk.attn.num_heads,\n",
    "#                     blk.attn.dropout,\n",
    "#                     batch_first=blk.attn.batch_first,\n",
    "#                 )\n",
    "#                 inj.set_parameters(blk.attn)\n",
    "#                 blk.attn = inj\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # ───────────────────────── 5.  LoRA injection helper ──────────────────────────\n",
    "\n",
    "\n",
    "# def add_lora(model: nn.Module, r=8, alpha=32, dropout=0.05):\n",
    "#     targets = {\n",
    "#         name.split(\".\")[-1]\n",
    "#         for name, m in model.named_modules()\n",
    "#         if isinstance(m, nn.Linear) and name.split(\".\")[-1] in {\"qkv\", \"proj\", \"c_proj\"}\n",
    "#     }\n",
    "#     cfg = LoraConfig(\n",
    "#         r=r,\n",
    "#         lora_alpha=alpha,\n",
    "#         lora_dropout=dropout,\n",
    "#         bias=\"none\",\n",
    "#         target_modules=sorted(targets),\n",
    "#         task_type=\"FEATURE_EXTRACTION\",\n",
    "#         # dtype=torch.bfloat16,\n",
    "#     )\n",
    "#     model = get_peft_model(model, cfg)\n",
    "#     model.print_trainable_parameters()  # log\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # ───────────────────────── 6.  Joint regression wrapper ───────────────────────\n",
    "# class ClipJointRegression(nn.Module):\n",
    "#     def __init__(self, alpha=0.4, huber_delta=1.0):\n",
    "#         super().__init__()\n",
    "#         self.clip, _, _ = create_model_and_transforms(CLIP_NAME)\n",
    "#         inject_linear_attention(self.clip, {\"transformer\"})\n",
    "#         self.clip = add_lora(self.clip, r=32, alpha=256)\n",
    "#         self.clip = self.clip.to(device)\n",
    "#         self.proj_dim = (\n",
    "#             self.clip.embed_dim\n",
    "#             if hasattr(self.clip, \"embed_dim\")\n",
    "#             else self.clip.visual.attnpool.c_proj.out_features\n",
    "#         )\n",
    "#         self.regressor = nn.Linear(self.proj_dim, 1)\n",
    "#         self.alpha = alpha\n",
    "#         self.huber = nn.HuberLoss(delta=huber_delta)\n",
    "\n",
    "#     def forward(self, pixel_values=None, input_ids=None, labels=None, **kw):\n",
    "#         img_f = self.clip.encode_image(pixel_values)\n",
    "#         txt_f = self.clip.encode_text(input_ids)\n",
    "#         img_f, txt_f = F.normalize(img_f, dim=-1), F.normalize(txt_f, dim=-1)\n",
    "#         logits = img_f @ txt_f.T\n",
    "#         loss_itc = F.cross_entropy(\n",
    "#             logits, torch.arange(img_f.size(0), device=logits.device)\n",
    "#         )\n",
    "#         preds = self.regressor(img_f).squeeze(-1)\n",
    "#         loss_reg = self.huber(preds, labels) if labels is not None else 0.0\n",
    "#         return {\n",
    "#             \"loss\": self.alpha * loss_itc + (1 - self.alpha) * loss_reg,\n",
    "#             \"predictions\": preds,\n",
    "#         }\n",
    "\n",
    "\n",
    "# # ───────────────────────── 7.  Collate & metric ───────────────────────────────\n",
    "# PAD_ID = 0\n",
    "\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     imgs, toks, labs = zip(*batch)\n",
    "#     return {\n",
    "#         \"pixel_values\": torch.stack(imgs),\n",
    "#         \"input_ids\": torch.stack(toks),\n",
    "#         \"attention_mask\": (torch.stack(toks) != PAD_ID).long(),\n",
    "#         \"labels\": torch.stack(labs),\n",
    "#     }\n",
    "\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     preds, labels = eval_pred\n",
    "#     return {\"corr\": pearsonr(labels.flatten(), preds.flatten())[0]}\n",
    "\n",
    "\n",
    "# # ───────────────────────── 8.  K-fold training routine ────────────────────────\n",
    "\n",
    "\n",
    "# def kfold_train(df: pd.DataFrame, n_splits=6, seed=42):\n",
    "#     kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "#     all_corr = []\n",
    "#     for fold, (tr_idx, vl_idx) in enumerate(kf.split(df), 1):\n",
    "#         print(f\"===== Fold {fold}/{n_splits} =====\")\n",
    "#         tr_ds = MultiViewDataset(\n",
    "#             df.iloc[tr_idx], [clip_preprocess] + [train_transform] * 5\n",
    "#         )\n",
    "#         vl_ds = ClipCnnDataset(df.iloc[vl_idx], val_transform)\n",
    "#         args = TrainingArguments(\n",
    "#             output_dir=f\"outputs/fold{fold}\",\n",
    "#             per_device_train_batch_size=1200,\n",
    "#             per_device_eval_batch_size=1200,\n",
    "#             gradient_accumulation_steps=1,\n",
    "#             num_train_epochs=20,\n",
    "#             learning_rate=2e-3,\n",
    "#             bf16=True,\n",
    "#             logging_steps=50,\n",
    "#             eval_strategy=\"epoch\",\n",
    "#             save_strategy=\"epoch\",\n",
    "#             remove_unused_columns=False,\n",
    "#         )\n",
    "#         model = ClipJointRegression().to(device)\n",
    "#         trainer = Trainer(\n",
    "#             model=model,\n",
    "#             args=args,\n",
    "#             train_dataset=tr_ds,\n",
    "#             eval_dataset=vl_ds,\n",
    "#             data_collator=collate_fn,\n",
    "#             compute_metrics=compute_metrics,\n",
    "#         )\n",
    "#         trainer.train()\n",
    "#         r = trainer.evaluate()[\"eval_corr\"]\n",
    "#         print(f\"Fold{fold}: r={r:.4f}\")\n",
    "#         all_corr.append(r)\n",
    "#     print(f\"Mean r: {np.mean(all_corr):.4f} ± {np.std(all_corr):.4f}\")\n",
    "#     return all_corr\n",
    "\n",
    "\n",
    "# # ───────────────────────── 9.  Entry-point helper ─────────────────────────────\n",
    "# kfold_train(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Fold 1/6 =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25710' max='25710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25710/25710 31:00:45, Epoch 29/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.582700</td>\n",
       "      <td>3.496542</td>\n",
       "      <td>0.263356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.054600</td>\n",
       "      <td>3.037796</td>\n",
       "      <td>0.289570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.870300</td>\n",
       "      <td>2.862105</td>\n",
       "      <td>0.328615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.792000</td>\n",
       "      <td>2.790551</td>\n",
       "      <td>0.339870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.755400</td>\n",
       "      <td>2.751549</td>\n",
       "      <td>0.343511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.734000</td>\n",
       "      <td>2.732594</td>\n",
       "      <td>0.336162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.710200</td>\n",
       "      <td>2.713570</td>\n",
       "      <td>0.340518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.696800</td>\n",
       "      <td>2.699706</td>\n",
       "      <td>0.344164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.686300</td>\n",
       "      <td>2.689655</td>\n",
       "      <td>0.342932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.678100</td>\n",
       "      <td>2.683159</td>\n",
       "      <td>0.342484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.677100</td>\n",
       "      <td>2.678162</td>\n",
       "      <td>0.342006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.668500</td>\n",
       "      <td>2.674905</td>\n",
       "      <td>0.341962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.662700</td>\n",
       "      <td>2.669977</td>\n",
       "      <td>0.342519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.660700</td>\n",
       "      <td>2.667128</td>\n",
       "      <td>0.343218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.661000</td>\n",
       "      <td>2.666277</td>\n",
       "      <td>0.342903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.657500</td>\n",
       "      <td>2.664189</td>\n",
       "      <td>0.341725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.657300</td>\n",
       "      <td>2.662282</td>\n",
       "      <td>0.343073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.649300</td>\n",
       "      <td>2.659921</td>\n",
       "      <td>0.344222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.652800</td>\n",
       "      <td>2.659753</td>\n",
       "      <td>0.344404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.653200</td>\n",
       "      <td>2.658880</td>\n",
       "      <td>0.342731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.648600</td>\n",
       "      <td>2.656724</td>\n",
       "      <td>0.344540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.649800</td>\n",
       "      <td>2.655865</td>\n",
       "      <td>0.344898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.650400</td>\n",
       "      <td>2.655798</td>\n",
       "      <td>0.345014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.649800</td>\n",
       "      <td>2.654852</td>\n",
       "      <td>0.345519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.651500</td>\n",
       "      <td>2.654505</td>\n",
       "      <td>0.345645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.649800</td>\n",
       "      <td>2.653677</td>\n",
       "      <td>0.345918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.645600</td>\n",
       "      <td>2.653447</td>\n",
       "      <td>0.346549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.646200</td>\n",
       "      <td>2.653496</td>\n",
       "      <td>0.345494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.648300</td>\n",
       "      <td>2.652868</td>\n",
       "      <td>0.346423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115' max='115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115/115 00:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1: Pearson r = 0.3465\n",
      "===== Fold 2/6 =====\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12013' max='25710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12013/25710 14:12:47 < 16:12:30, 0.23 it/s, Epoch 14/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.569400</td>\n",
       "      <td>3.460780</td>\n",
       "      <td>0.262139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.053900</td>\n",
       "      <td>3.019855</td>\n",
       "      <td>0.287640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.867600</td>\n",
       "      <td>2.854257</td>\n",
       "      <td>0.316389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.788900</td>\n",
       "      <td>2.786146</td>\n",
       "      <td>0.325813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.752200</td>\n",
       "      <td>2.750420</td>\n",
       "      <td>0.330478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.730600</td>\n",
       "      <td>2.727672</td>\n",
       "      <td>0.329574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.713100</td>\n",
       "      <td>2.715343</td>\n",
       "      <td>0.325906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.693000</td>\n",
       "      <td>2.701537</td>\n",
       "      <td>0.331666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.685600</td>\n",
       "      <td>2.692324</td>\n",
       "      <td>0.329055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.678800</td>\n",
       "      <td>2.684760</td>\n",
       "      <td>0.330016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.675300</td>\n",
       "      <td>2.678516</td>\n",
       "      <td>0.333145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.665200</td>\n",
       "      <td>2.674098</td>\n",
       "      <td>0.333555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.665700</td>\n",
       "      <td>2.671757</td>\n",
       "      <td>0.332859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import pearsonr\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from open_clip import create_model_and_transforms, get_tokenizer\n",
    "from transformers.utils import logging as hf_logging\n",
    "\n",
    "# hf_logging.set_verbosity_info()  # ← ロギングも見やすく\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0. 基本設定\n",
    "# -----------------------------------------------------------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "CLIP_NAME = \"hf-hub:timm/resnet50x4_clip.openai\"\n",
    "\n",
    "# CLIP の本体と前処理、トークナイザ\n",
    "clip_backbone, _, clip_preprocess = create_model_and_transforms(CLIP_NAME)\n",
    "tokenizer = get_tokenizer(CLIP_NAME)\n",
    "\n",
    "\n",
    "import torchvision.transforms.v2 as T\n",
    "import torch\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 0.  CLIP が公開している mean / std を取得\n",
    "#     create_model_and_transforms が返す clip_preprocess の\n",
    "#     Normalize から値を抜き出して再利用します\n",
    "# ───────────────────────────────────────────────\n",
    "norm_layer = next(\n",
    "    t for t in clip_preprocess.transforms if isinstance(t, transforms.Normalize)\n",
    ")\n",
    "CLIP_MEAN, CLIP_STD = norm_layer.mean, norm_layer.std  # list[3]*\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────\n",
    "# 1.  v2 版の transform を組む\n",
    "# ───────────────────────────────────────────────\n",
    "def build_train_transform(device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    TorchVision v2 版データ拡張パイプライン。\n",
    "    device=\"cuda\" とすると augment も GPU で実行できます。\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        T.Compose(\n",
    "            [\n",
    "                # --- PIL → Tensor(uint8, CxHxW) ---\n",
    "                T.ToImage(),\n",
    "                # --- 基本前処理 ---\n",
    "                T.Resize((288, 288)),\n",
    "                T.CenterCrop(288),\n",
    "                # --- データ拡張 ---\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.RandomAffine(\n",
    "                    degrees=20,\n",
    "                    translate=(0.2, 0.2),\n",
    "                    scale=(0.7, 1.2),\n",
    "                    fill=(255, 255, 255),\n",
    "                ),\n",
    "                T.GaussianBlur(kernel_size=5, sigma=(0.01, 4.0)),\n",
    "                T.ColorJitter(\n",
    "                    brightness=0.075,\n",
    "                    contrast=0.0,\n",
    "                    saturation=0.03,\n",
    "                    hue=0.03,\n",
    "                ),\n",
    "                # --- Tensor float32 化 & 正規化 (CLIP) ---\n",
    "                T.ToDtype(torch.float32, scale=True),\n",
    "                T.Normalize(CLIP_MEAN, CLIP_STD),\n",
    "            ]\n",
    "        ).to(device)  # ← \"cuda\" を渡せば GPU オーグメント\n",
    "    )\n",
    "\n",
    "train_transform = build_train_transform(\"cuda\")  # データ拡張付き\n",
    "val_transform = clip_preprocess  # 検証はデフォルトのみ\n",
    "\n",
    "orig_transform = clip_preprocess  # = val_transform と同じ\n",
    "aug_transform = train_transform\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. データセット\n",
    "# -----------------------------------------------------------------------------\n",
    "class ClipCnnDataset(Dataset):\n",
    "    \"\"\"画像・キャプション・連続値ラベルを扱うデータセット\"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform if transform is not None else clip_preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        image = self.transform(image)  # → Tensor[3, H, W] - don't unsqueeze\n",
    "\n",
    "        # テキストをトークンに変換（OpenCLIP は固定長 77、pad_id=0）\n",
    "        text_tokens = tokenizer(row[\"caption\"]).squeeze(0)\n",
    "\n",
    "        return image, text_tokens, torch.tensor(row[\"res_L\"], dtype=torch.float32)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. モデル\n",
    "# -----------------------------------------------------------------------------\n",
    "class ClipJointRegression(nn.Module):\n",
    "    \"\"\"CLIP のコントラスト学習 + 画像特徴による回帰を同時に行うモデル\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone_name: str = CLIP_NAME,\n",
    "        alpha: float = 0.4,\n",
    "        huber_delta: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.clip, _, _ = create_model_and_transforms(backbone_name)\n",
    "        self.clip = self.clip.to(device)\n",
    "        # OpenCLIP models store the dimension in different attributes\n",
    "        if hasattr(self.clip, 'embed_dim'):\n",
    "            self.proj_dim = self.clip.embed_dim\n",
    "        elif hasattr(self.clip, 'transformer'):\n",
    "            self.proj_dim = self.clip.transformer.width\n",
    "        else:\n",
    "            # For ResNet CLIP models, get dim from the visual projection\n",
    "            self.proj_dim = self.clip.visual.attnpool.c_proj.out_features\n",
    "        # print(self.proj_dim)\n",
    "        self.regressor = nn.Linear(self.proj_dim, 1)\n",
    "        self.alpha = alpha\n",
    "        self.huber = nn.HuberLoss(delta=huber_delta)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values=None,  # 画像 (B,3,H,W)\n",
    "        input_ids=None,  # テキストトークン (B,77)\n",
    "        attention_mask=None,  # 使わなくても受け取る\n",
    "        labels=None,  # 連続値ラベル (B)\n",
    "    ):\n",
    "        # --- ITC (image–text contrast) ---\n",
    "        image_features = self.clip.encode_image(pixel_values)\n",
    "        text_features = self.clip.encode_text(input_ids)\n",
    "\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        logits = image_features @ text_features.T\n",
    "        itc_labels = torch.arange(image_features.size(0), device=logits.device)\n",
    "        loss_itc = nn.CrossEntropyLoss()(logits, itc_labels)\n",
    "\n",
    "        # --- 回帰 ---\n",
    "        preds = self.regressor(image_features).squeeze(-1)\n",
    "        loss_reg = self.huber(preds, labels) if labels is not None else 0.0\n",
    "\n",
    "        loss = self.alpha * loss_itc + (1.0 - self.alpha) * loss_reg\n",
    "        return {\"loss\": loss, \"predictions\": preds}\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. 補助関数\n",
    "# -----------------------------------------------------------------------------\n",
    "PAD_ID = 0\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    batch: list[Tuple[Tensor[C,H,W], Tensor[77], Tensor[]]]\n",
    "    \"\"\"\n",
    "    # それぞれをリストに展開\n",
    "    images, text_tokens, labels = zip(*batch)\n",
    "    # 画像・トークン・ラベルをテンソルにまとめる\n",
    "    pixel_values = torch.stack(images)  # [B, 3, H, W]\n",
    "    input_ids = torch.stack(text_tokens)  # [B, 77]\n",
    "    labels = torch.stack(labels)  # [B]\n",
    "\n",
    "    # attention_mask を付けたい場合（任意）\n",
    "    attention_mask = (input_ids != PAD_ID).long()  # [B, 77]\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,  # ←不要なら削除して OK\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = preds.flatten()\n",
    "    labels = labels.flatten()\n",
    "    corr, _ = pearsonr(labels, preds)\n",
    "    return {\"corr\": corr}\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class MultiViewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    1 画像 × len(transforms_list) 通りの view を返す。\n",
    "    例）[orig_transform] + [aug_transform]*4 なら 5 倍。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, transforms_list):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transforms = transforms_list\n",
    "        self.n_views = len(transforms_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df) * self.n_views\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = idx // self.n_views  # 何番目の画像か\n",
    "        view_idx = idx % self.n_views  # 何番目の変換か\n",
    "        row = self.df.iloc[img_idx]\n",
    "\n",
    "        # 画像を読み込み、該当する transform を適用\n",
    "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
    "        image = self.transforms[view_idx](image)  # Remove unsqueeze\n",
    "\n",
    "        text_tokens = tokenizer(row[\"caption\"]).squeeze(0)\n",
    "        label = torch.tensor(row[\"res_L\"], dtype=torch.float32)\n",
    "        return image, text_tokens, label\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def kfold_train(df: pd.DataFrame, n_splits: int = 6, seed: int = 42, repeat: int = 5):\n",
    "    full_dataset = ClipCnnDataset(df)\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    all_corrs = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(range(len(full_dataset))), 1):\n",
    "        print(f\"===== Fold {fold}/{n_splits} =====\")\n",
    "        train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        transforms_list = [orig_transform] + [aug_transform] * 5\n",
    "        train_dataset = MultiViewDataset(train_df, transforms_list)\n",
    "\n",
    "        val_dataset = ClipCnnDataset(val_df, transform=val_transform)\n",
    "\n",
    "            # For newer versions of transformers (4.0.0+)\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"./outputs/clip/fold{fold}\",\n",
    "            per_device_train_batch_size=260,\n",
    "            per_device_eval_batch_size=260,\n",
    "            learning_rate=1e-4,\n",
    "            num_train_epochs=30,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1,  # Keep only the best model\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"corr\",\n",
    "            greater_is_better=True,\n",
    "            logging_steps=100,\n",
    "            bf16=torch.cuda.is_available(),\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_num_workers=12,  # CPU コア数と相談して増減\n",
    "            dataloader_pin_memory=True,  # A100 など GPU があるなら True 推奨\n",
    "            dataloader_persistent_workers=True,  # workers をエポック間で使い回す (optional)\n",
    "            disable_tqdm=False,\n",
    "            gradient_accumulation_steps=4,\n",
    "        )\n",
    "\n",
    "\n",
    "        model = ClipJointRegression(alpha=0.4, huber_delta=1.0).to(device)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=collate_fn,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        # 学習\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "        fold_corr = metrics[\"eval_corr\"]\n",
    "        print(f\"Fold {fold}: Pearson r = {fold_corr:.4f}\")\n",
    "        all_corrs.append(fold_corr)\n",
    "\n",
    "    mean_corr = np.mean(all_corrs)\n",
    "    std_corr = np.std(all_corrs)\n",
    "    print(f\"\\nFinished {n_splits}-Fold CV. Mean r = {mean_corr:.4f} ± {std_corr:.4f}\")\n",
    "    return all_corrs\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. 学習実行\n",
    "# -----------------------------------------------------------------------------\n",
    "all_corrs = kfold_train(df, n_splits=6, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eda import save_intermediate_outputs, register_hooks, save_layer_matrixs\n",
    "\n",
    "layer_num = register_hooks(model)\n",
    "print(layer_num)\n",
    "save_dir = os.path.join(\n",
    "    ROOT_PATH,\n",
    "    \"tmp\",\n",
    "    \"clip\",\n",
    "    \"intermediate_feature\",\n",
    ")\n",
    "# layer_num = 81\n",
    "save_intermediate_outputs(model, dataset, save_dir, device)\n",
    "save_layer_matrixs(save_dir, layer_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = \"v2\"\n",
    "image_features_list = []\n",
    "\n",
    "for image, label in tqdm(dataset):\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    image_features_list.append(image_features)\n",
    "\n",
    "image_features = torch.cat(image_features_list)\n",
    "print(image_features.shape)\n",
    "torch.save(image_features, os.path.join(DATA_PATH, \"output\", \"clip\", VERSION, \"image_features.pt\"))\n",
    "image_features = torch.load(\n",
    "    os.path.join(DATA_PATH, \"output\", \"clip\", VERSION, \"image_features.pt\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge regression\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "def pearson_scorer(y_true, y_pred):\n",
    "    corr, _ = pearsonr(y_true, y_pred)\n",
    "    return np.mean(corr)\n",
    "\n",
    "\n",
    "image_dir = os.path.join(ROOT_PATH, \"Database\")\n",
    "pearson_sklearn = make_scorer(pearson_scorer, greater_is_better=True)\n",
    "X = image_features.cpu().numpy()\n",
    "h = 5\n",
    "skf = StratifiedKFold(n_splits=h, shuffle=True, random_state=1)\n",
    "kf = KFold(n_splits=h, shuffle=True, random_state=3)\n",
    "\n",
    "# pca = make_pipeline(StandardScaler(), PCA(n_components=0.98))\n",
    "# X = pca.fit_transform(image_features.cpu().numpy())\n",
    "result = {}\n",
    "for i, res in enumerate([res_L_mean, res_H_mean, res_T_mean, image_brightness]):\n",
    "    result[res.name] = {\n",
    "        \"obesity\": {\n",
    "            \"y_tests\": [],\n",
    "            \"y_preds\": [],\n",
    "        },\n",
    "        \"normal\": {\n",
    "            \"y_tests\": [],\n",
    "            \"y_preds\": [],\n",
    "        },\n",
    "    }\n",
    "    # for is_obesity, y in res.groupby(\"is_obesity\"):\n",
    "    y = res.groupby(\"img\").mean() if res.name != \"brightness\" else res\n",
    "    # dataset = ImageDataset(image_dir, y)\n",
    "    labels = y.values\n",
    "    # n_bins = 8  # ビンの数（例: 4つのカテゴリに分割）\n",
    "    # bins = np.linspace(1, 8, n_bins + 1)  # ビンの境界を定義\n",
    "    # binned_labels = np.digitize(labels, bins) - 1\n",
    "    # normal_score = 0\n",
    "    # if not is_obesity:\n",
    "    #     normal_score_list = []\n",
    "    pbar = tqdm(\n",
    "        # enumerate(skf.split(np.zeros(len(binned_labels)), binned_labels)),\n",
    "        enumerate(kf.split(np.zeros(len(labels)), labels)),\n",
    "        total=h,\n",
    "        leave=False,\n",
    "    )\n",
    "    for j, (train_idx, val_idx) in pbar:\n",
    "        # Train and validation subsets\n",
    "        print(y.name)\n",
    "        # print(\"肥満\" if is_obesity else \"健常\")\n",
    "        print(len(y))\n",
    "        X_train, X_test = X[train_idx], X[val_idx]\n",
    "        y_train, y_test = labels[train_idx], labels[val_idx]\n",
    "        ridge = make_pipeline(Ridge(alpha=1.0))\n",
    "        # random search\n",
    "        # param_distributions = {\"ridge__alpha\": loguniform(1e-3, 1e3)}\n",
    "\n",
    "        # search = RandomizedSearchCV(\n",
    "        #     ridge,\n",
    "        #     param_distributions,\n",
    "        #     n_iter=20,\n",
    "        #     cv=5,\n",
    "        #     n_jobs=4,\n",
    "        #     random_state=42,\n",
    "        #     scoring=pearson_sklearn,\n",
    "        # )\n",
    "\n",
    "        ridge.fit(X_train, y_train)\n",
    "        # scores = cross_val_score(ridge, X, labels, cv=5, scoring=pearson_sklearn)\n",
    "        # print(np.mean(scores))\n",
    "        # print(ridge.best_params_)\n",
    "        # print(\"best score\", search.best_score_)\n",
    "        # best_model = search.best_estimator_\n",
    "        y_pred = ridge.predict(X_test)\n",
    "        result[res.name][\"normal\"][\"y_preds\"].append(\n",
    "            y_pred\n",
    "        )\n",
    "        result[res.name][\"normal\"][\"y_tests\"].append(\n",
    "            y_test\n",
    "        )\n",
    "    print(np.mean([pearson_scorer(\n",
    "                y_test,\n",
    "                result[res.name][\"normal\"][\"y_preds\"][i],\n",
    "            ) for i, y_test in enumerate(result[res.name][\"normal\"][\"y_tests\"])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 9))\n",
    "for i, (name, value) in enumerate(result.items()):\n",
    "    print(name)\n",
    "    labels = []\n",
    "    # if name == \"res_L\":\n",
    "    #     with open(os.path.join(DATA_PATH, \"output\", \"clip_res_L_score_v2.pkl\"), \"wb\") as f:\n",
    "    #         pickle.dump(\n",
    "    #             {\n",
    "    #                 \"obesity\": {\n",
    "    #                     \"y_tests\": value[\"obesity\"][\"y_tests\"],\n",
    "    #                     \"y_preds\": value[\"obesity\"][\"y_preds\"],\n",
    "    #                     \"score\": pearson_scorer(\n",
    "    #                         np.concatenate(value[\"obesity\"][\"y_tests\"]),\n",
    "    #                         np.concatenate(value[\"obesity\"][\"y_preds\"]),\n",
    "    #                     ),\n",
    "    #                 },\n",
    "    #                 \"normal\": {\n",
    "    #                     \"y_tests\": value[\"normal\"][\"y_tests\"],\n",
    "    #                     \"y_preds\": value[\"normal\"][\"y_preds\"],\n",
    "    #                     \"score\": pearson_scorer(\n",
    "    #                         np.concatenate(value[\"normal\"][\"y_tests\"]),\n",
    "    #                         np.concatenate(value[\"normal\"][\"y_preds\"]),\n",
    "    #                     ),\n",
    "    #                 },\n",
    "    #             },\n",
    "    #             f,\n",
    "    #         )\n",
    "    # for type_name, y_dict in value.items():\n",
    "    # if name == \"brightness\" and type_name == \"obesity\":\n",
    "    #     continue\n",
    "    y_dict = value[\"normal\"]\n",
    "    y_test = np.concatenate(y_dict[\"y_tests\"])\n",
    "    y_pred = np.concatenate(y_dict[\"y_preds\"])\n",
    "    score = pearson_scorer(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    labels.append(f\"normal : {round(score, 3)}\")\n",
    "\n",
    "    axes[i].scatter(\n",
    "        y_test,\n",
    "        y_pred,\n",
    "        label=f\"{name}_{'健常'}\",\n",
    "        alpha=0.5,\n",
    "        s=10,\n",
    "    )\n",
    "    handle, _ = axes[i].get_legend_handles_labels()\n",
    "\n",
    "    axes[i].legend(\n",
    "        handle,\n",
    "        labels,\n",
    "        loc=\"upper left\",\n",
    "    )\n",
    "    axes[i].set_xlabel(\"True\")\n",
    "    axes[i].set_ylabel(\"Predicted\")\n",
    "    # if y.name == \"brightness\":\n",
    "    #     axes[i].set_xticks(\n",
    "    #         np.arange(\n",
    "    #             round(y_test.min() - 0.1, 2), round(y_test.max() + 0.1, 2), 0.1\n",
    "    #         )\n",
    "    #     )\n",
    "    #     axes[i].set_yticks(\n",
    "    #         np.arange(\n",
    "    #             round(y_test.min() - 0.1, 2), round(y_test.max() + 0.1, 2), 0.1\n",
    "    #         )\n",
    "    #     )\n",
    "    # else:\n",
    "    #     axes[i].set_xticks(np.arange(1, 9, 1))\n",
    "    #     axes[i].set_yticks(np.arange(1, 9, 1))\n",
    "    print(round(mse, 3), round(score, 3))\n",
    "\n",
    "    match name:\n",
    "        case \"res_L\":\n",
    "            title = \"likablity\"\n",
    "        case \"res_H\":\n",
    "            title = \"healthiness\"\n",
    "        case \"res_T\":\n",
    "            title = \"tastiness\"\n",
    "        case \"brightness\":\n",
    "            title = \"brightness\"\n",
    "    axes[i].set_title(title + \" MSE \" + str(round(mse, 3)))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foodreward-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
