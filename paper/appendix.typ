== fMRIの仕組み <app1>
=== 機能的磁気共鳴画像法（functional Magnetic Resonance Imaging; fMRI）

fMRIは、脳活動に伴う血流動態の変化を非侵襲的に測定する神経画像法である。fMRIは、強力な磁場を利用して脳の血液酸素レベル依存信号（Blood Oxygen Level-Dependent; BOLD信号）と呼ばれる血液酸素量から神経活動を間接的に測定する技術である @Fukuyama2016 。 以下、本セクションの技術的説明は同文献に基づく。

=== MRI信号の仕組み：核磁気共鳴と緩和現象

MRIは、強力な静磁場（通常1.5〜7テスラ）中に置かれた陽子（プロトン）の核磁気共鳴させることで、電磁場を発生させそれをMR信号として受信する。fMRIでの静磁場とは、磁場が広い領域均一であり（空間的に均一）、磁場が時間によって変化しない（時間的に均一）である磁場のことを指す。静磁場中でプロトンは磁場方向に整列するが、この状態にラジオ波コイルでラーモア周波数（Larmor frequency）を照射すると、プロトンはエネルギーを吸収して励起状態へ遷移する。ラーモア周波数とは、プロトンを共鳴することができる周波数のことである。RFパルスを停止すると、プロトンは平衡状態（元の状態）に戻る過程で電磁波を放出する。この信号を検出して画像化するのがMRIの基本原理である。

プロトンが平衡状態に戻る過程は緩和と呼ばれ、T1緩和とT2緩和の2つの独立した過程がある。T1緩和（縦緩和または spin-lattice relaxation）は、励起されたプロトンが周囲の格子（lattice）にエネルギーを放出して、縦磁化（静磁場方向の磁化）が回復する過程である。T1緩和時間は組織によって異なり、灰白質では約1秒、白質では約800ミリ秒程度である。一方、T2緩和（横緩和または spin-spin relaxation）は、プロトン間の相互作用により位相がずれ、静磁場に垂直な磁化が減衰する過程である。T2緩和時間は組織の微細構造や水分含有量に依存し、灰白質では約100ミリ秒、白質では約80ミリ秒程度である。このため、T1画像はT1緩和時間の違いを利用して組織コントラストを生成し、脂肪含有量の多い組織（例えば白質）が高信号となる。一方、T2画像はT2緩和時間の違いを利用し、水分含有量の多い組織（例えば脳脊髄液）が高信号となる。

さらに、T2$*$緩和はT2緩和に加えて、局所的な磁場不均一性による信号減衰を含む。T2$*$緩和時間はT2緩和時間よりも短く、組織の磁気的性質や血液中の酸素化状態に敏感である。この特性を利用して、fMRIではT2$*$強調画像を取得し、BOLD信号を検出する。

=== BOLD信号の生成メカニズム

神経活動が生じると、活動領域への血流が増加し、デオキシヘモグロビンがオキシヘモグロビンに置き換わる。オキシヘモグロビン（oxyHb）は反磁性であるのに対し、デオキシヘモグロビン（deoxyHb）は常磁性である。この磁気的性質の違いにより、局所的な磁場の均一性が変化し、T2$*$緩和時間に影響を及ぼす。具体的には、神経活動に伴ってデオキシヘモグロビンの相対濃度が減少すると、局所磁場の均一性が高まり、T2$*$緩和時間が延長する結果、MR信号強度が増加する。この信号変化がBOLD信号として検出される。

fMRIデータは、脳全体を3次元的に分割したボクセル（voxel：体積要素）ごとに時系列のMR信号として取得される。EPIシーケンスにより、脳全体のボクセル信号を1〜3秒ごとに繰り返し取得することで、時間的に変動するBOLD信号を記録する。

=== 血行動態応答と時間特性

BOLD信号は神経活動に対して間接的な指標であり、血行動態応答（hemodynamic response）を反映している。神経活動の開始後、BOLD信号は約2秒の遅延を経て上昇し始め、4〜6秒後にピークに達する。その後、信号は徐々に減少し、ベースライン付近で一時的なアンダーシュート（post-stimulus undershoot）を示した後、約12〜20秒で元のレベルに戻る。この時間経過は血行動態応答関数（hemodynamic response function; HRF）によって、イベントなどをモデル化することができる。この関数から、統計解析を行うことで、BOLD信号変化に対するイベントなどが与える効果を推定できる。

fMRIの空間解像度は通常1〜3 mm程度であり、時間解像度は1〜3秒程度である。この時空間解像度により、fMRIは非侵襲的に脳の特定領域における活動パターンを測定することができるため、ヒトの認知機能や感覚処理、意思決定などの神経基盤を解明するために広く利用されている。

== ニューラルネットワークモデルの構造 <app2>
ニューラルネットワーク（Neural Network; NN）は、生物の神経系に着想を得た計算モデルであり、情報処理やパターン認識に広く用いられている。NNは、複数の層にわたる相互接続されたノード（ニューロン）で構成され、各ノードは入力信号を受け取り、重み付けされた和を計算し、活性化関数を通じて出力信号を生成する。

ニューラルネットワークは、入力層、中間層（隠れ層）、出力層の3つの主要な層で構成される。入力層は外部からのデータを受け取り、中間層は複数のニューロンで構成され、入力データの特徴を抽出し、非線形変換を行う。出力層は最終的な予測や分類結果を生成する。各ニューロンは、前の層からの入力信号に対して重みを適用し、バイアス項を加えた後、活性化関数を通じて出力信号を生成する。活性化関数には、シグモイド関数、ReLU関数、ソフトマックス関数などがある。活性化関数は、各ニューロンの重みを一定の範囲に制限することによって、NNの計算量が増加するのを防ぐ役割を果たす。

#grid(
  columns: 2,
  [#figure(
    image("./image/dnn.png", width: 75%),
    supplement: "図",
    caption: [ニューラルネットワークの基本構造。入力層、中間層（隠れ層）、出力層から構成される。各ニューロンは、前の層からの入力信号に対して重みを適用し、バイアス項を加えた後、活性化関数を通じて出力信号を生成する。],
  ) <fig:nn_structure>],
  [
    #figure(
      // Replace the rectangle with an image once available:
      image("./image/activation.png", width: 90%),
      supplement: "図",
      caption: [ニューラルネットワークにおける代表的な活性化関数の例。左からシグモイド関数、ReLU関数、ソフトマックス関数。各関数は、ニューロンの出力信号を生成するために使用される。],
    ) <fig:nn_activation>],
)

ニューラルネットワークの訓練は、主に教師あり学習に基づいて行われる。訓練データセットを用いて、ネットワークの重みとバイアスを最適化する。一般的な訓練手法として、誤差逆伝播法（Backpropagation）と勾配降下法（Gradient Descent）がある。誤差逆伝播法は、出力層で計算された誤差を中間層および入力層に逆伝播させ、各ニューロンの重みとバイアスの勾配を計算する。勾配降下法は、これらの勾配を用いて重みとバイアスを更新し、損失関数を最小化する。損失関数には、平均二乗誤差（Mean Squared Error; MSE）や交差エントロピー損失（Cross-Entropy Loss）などがある。訓練プロセスは、エポック（Epoch）と呼ばれる複数の反復で行われ、各エポックで全ての訓練データがネットワークに入力される。

== ディープニューラルネットワークモデル（DNN）<app3>
=== CNNアーキテクチャ
畳み込みニューラルネットワーク（CNN）は、特に画像データの処理に優れた性能を発揮するディープラーニングモデルである。CNNは、畳み込み層、プーリング層、全結合層から構成されており、画像の特徴を自動的に抽出する能力を持つ。畳み込み層は、入力画像に対して畳み込みフィルターを適用し、特徴マップを生成する。プーリング層は、特徴マップの空間的次元を削減し、計算量を軽減するとともに、位置不変性を持たせる役割を果たす @Kriegeskorte2015。全結合層は、最終的な分類結果を出力するために、抽出された特徴を用いている。また、CNNは多層構造で畳み込みを行う視覚野の階層的処理を模倣しており、初期層はエッジやテクスチャなどの低次特徴を抽出し、後続層はオブジェクトの形状やカテゴリなどの高次特徴を抽出する @Yamins2014-it; @Kriegeskorte2015。

#figure(
  image("./image/cnn.png", width: 90%),
  supplement: "図",
  caption: [@simonyan2015; から作成。畳み込みニューラルネットワーク（CNN）の基本構造の例（VGG16）。畳み込み層、プーリング層、全結合層から構成される。畳み込み層は入力画像に対して畳み込みフィルターを適用し、特徴マップを生成する。プーリング層は特徴マップの空間的次元を削減し、計算量を軽減するとともに、位置不変性を持たせる役割を果たす。全結合層は最終的な分類結果を出力するために、抽出された特徴を用いる。],
) <fig:cnn_structure>

#figure(
  image("./image/convolution.png", width: 100%, height: 90pt, fit: "contain"),
  supplement: "図",
  caption: [畳み込み操作の例。入力画像に対して畳み込みフィルターを適用し、特徴マップを生成する。フィルターは画像の局所的なパターンを検出するために使用される。],
) <fig:convolution_operation>


=== Transformerアーキテクチャ
トランスフォーマー（Transformer）は、自然言語処理タスクにおいて高い性能を示すディープラーニングモデルであり、自己注意メカニズムを用いて入力シーケンス内の異なる位置の情報を動的に重み付けする。トランスフォーマーは、エンコーダーとデコーダーの2つの主要なコンポーネントで構成されており、エンコーダーは入力シーケンスを処理し、デコーダーは出力シーケンスを生成する。トランスフォーマーは、大規模なテキストデータセットで学習され、文の生成、質問応答、翻訳などのタスクで優れた性能を示している。

#figure(
  image("./image/transformer.png", width: 60%),
  supplement: "図",
  caption: [トランスフォーマーの基本構造。エンコーダーとデコーダーの2つの主要なコンポーネントで構成されており、エンコーダーは入力シーケンスを処理し、デコーダーは出力シーケンスを生成する。自己注意メカニズムを用いて入力シーケンス内の異なる位置の情報を動的に重み付けする @Vaswani2017[p.3, Figure 1];。],
) <fig:transformer_structure>

マルチヘッド自己注意（Multi-Head Self-Attention; MHSA）は、トランスフォーマーの主要な構成要素であり、入力シーケンス内の異なる位置の情報を同時に処理する能力を持つ。MHSAは、複数の注意ヘッドを使用して、入力シーケンスの異なる部分に焦点を当てることができる。各注意ヘッドは、クエリ（Query）、キー（Key）、バリュー（Value）の3つのベクトルを生成し、これらを用いて注意重みを計算する。注意重みは、入力シーケンス内の各位置に対する重要度を表し、これを用いてバリューを加重平均することで、出力ベクトルを生成する。MHSAは、複数の注意ヘッドからの出力を結合し、線形変換を適用して最終的な出力を得る。例えば、エンコーダーでは、
$X$ を入力とすると、各注意ヘッド$i$に対して以下の計算が行われる。
$
  Q_i = X W_i^Q, K_i = X W_i^K, V_i = X W_i^V
$
$
  "head"_i = "Attention" (Q_i, K_i, V_i) = "softmax" ((Q_i K_i^T) / sqrt(d_k)) V_i
$
ここで、$W_i^Q, W_i^K, W_i^V$ はそれぞれクエリ、キー、バリューの重み行列であり、$d_k$ はキーの次元数である。最終的なMHAの出力は以下のように計算される。
$
  "MHA" (Q, K, V) = "Concat"("head"_1, dots, "head"_h)W^O
$

具体例を挙げると、入力シーケンスが「The cat sat on the mat」である場合、$Q_i$はその単語自身の情報、$K_i$は他の単語の情報、$V_i$は単語の実際の意味情報を保持している。そのため、「cat」が「sat」と強く関連していて、$q_"cat"$と$k_"sat"$の内積が大きくなり、$v_"cat"$の意味情報が強調される。これらを複数のヘッドで同時に処理することにより、さまざまな角度からの意味情報を捉えることができる。

== 各層の活性化パターン分析 <app4>
層 $ell$ ($ell = 1, 2, dots, L$) に対して，$i$ 番目の画像($i = 1, 2, dots, N$) を入力したときの
*フラット化された活性化ベクトル*を
$
  bold(h)_ell^(i) in RR^(d_ell)
$
とする。ここで、$d_ell$ は層 $ell$ のフラット化後の次元数。
これらを $N$ 枚分を並べた行列を
$
  bold(H)_ell
  =
  mat(
    (bold(h)_ell^(1))^T;
    (bold(h)_ell^(2))^T;
    dots.v;
    (bold(h)_ell^(N))^T
  )
  in RR^(N times d_ell)
$
と定義する。

=== PCAによる次元削減

各層 $ell$ ごとに活性化パターン行列 $bold(H)_ell$ に対して主成分分析（PCA）を実施し、累積寄与率が80%に達するまで主成分を選択した。選択された主成分の数を $M_ell$ とし、対応する射影行列を $bold(W)_(ell, "PCA") in RR^(d_ell times M_ell)$ と定義する。固有値が大きい上位 $M_ell$ 個の主成分を用いて、データを低次元空間へ射影する。
各次元の平均を 0 にするために、$bold(H)_ell$ の各列から対応する列平均を引いて中心化をする。中心化した行列 $tilde(bold(H))_ell$ の共分散行列 $C$ を 固有値分解あるいは特異値分解 (SVD) により対角化する。
$
  C = 1/n tilde(bold(H))_ell^T tilde(bold(H))_ell = bold(W) Lambda bold(W)^T,
$

ここで、$Lambda = "diag"(lambda_1, lambda_2, dots, lambda_(d_ell))$ は固有値（分散の大きさ）を対角に並べた対角行列、$bold(W) = [bold(w)_1, bold(w)_2, dots, bold(w)_(d_ell)]$ の各列 $bold(w)_i$ は対応する固有ベクトル（主成分ベクトル）を意味する。

$
  bold(Z)_ell = tilde(bold(H))_ell bold(W)_(ell, "PCA")
  quad in quad RR^(N times M_ell)
$
これにより、$RR^(d_ell)$ から $RR^(M_ell) ( M_ell << d_ell)$ へと次元削減が行われる。

=== Ridge回帰による画像特徴予測
*各画像属性*（$bold(y)_i$ の各成分）*ごとに* 層 $ell$ の特徴量 $tilde(bold(H))_ell$ を用いて *Ridge回帰* を行い，回帰係数を求める。すなわち，属性 $k$ に対応する目的変数を
$
  bold(y)^(k) =
  mat(
    y_(1,k);
    y_(2,k);
    dots.v;
    y_(N,k)
  )
  quad (in RR^N)
$
とおき，その回帰係数ベクトル（あるいは行列）を $bold(b)_ell^(k)$ とする。すると，属性 $k$ に対する Ridge回帰の目的関数は
$
  min_(bold(b)_ell^(k)) norm(bold(y)^(k) - tilde(bold(H))_ell bold(b)_ell^(k))_2^2 + lambda norm(bold(b)_ell^(k))_2^2
$
となる（$norm(dot)_2$ はユークリッドノルム，$lambda >= 0$ は正則化パラメータ）。*画像属性ごとに別々に回帰を行う*ことで，それぞれの属性に合わせた係数を推定した。

画像を学習データとテストデータに分け、学習データで８分割クロスバリデーションのグリットサーチを行い最適な正則化パラメータを求めた。正則化パラメータで学習したモデルで層ごとに予測精度を算出し、どの層がどの特徴をよく表現しているかを評価した。予測精度はピアソンの相関係数を使用した。栄養属性（protein_100g、fat_100g、carbs_100g、grams_total）については、一部の画像で値が欠損していたため、これらの属性の予測では欠損値を含む画像を除外して分析を行った。

== fMRIにおける一般化線型モデル（GLM）分析 <app5>
fMRIデータに対して一般化線型モデル（Generalized Linear Model; GLM）を適用し、各ボクセルのBOLD信号変動を説明することを提案した。@Friston1995

GLMは、以下の形式で表される。
$
  bold(y)_i = bold(x)_i beta + e_i
$
ここで、$bold(y)_i$ はボクセル $i$ のBOLD信号の時系列データ、$bold(x)_i$ は設計行列、$beta$ は回帰係数ベクトル、$e_i$ は誤差項である。デザイン行列 $bold(x)_i$ には、実験条件や刺激イベントに対応する説明変数が含まれ、各説明変数は血行動態応答関数（Hemodynamic Response Function; HRF）で畳み込まれる。

ここで、自己相関を考えるために
$
  e_i ~ N(0, sigma^2_i V)
$
と仮定する。$V$ は自己相関を表す共分散行列であり、$sigma^2_i$ はボクセル $i$ の誤差の分散である。$sigma^2_i V$を詳しく見ると

$
  sigma^2 V = mat(
    sigma^2 V_(11), sigma^2 V_(12), dots;
    sigma^2 V_(21), sigma^2 V_(22), dots;
    dots.v, dots.v, dots.down
  )
$
のように表され、$V_(i,j)$ は時点 $i$ と時点 $j$ の自己相関を示す要素である。Vを推定した後、GLMの自己相関を考慮したモデルは$W=V^(-1/2)$とすると以下のように変形できる。
$
  W bold(y)_i = W bold(x)_i beta + W e_i
$

これは、独立同一分布を持つ誤差項に変換するために$W=V^(-1/2)$をかけたものであり、以下のように誤差項の共分散が単位行列になる。

$
  "Cov"(W e_i) = W dot "Cov"(e_i) dot W^T = V^(-1/2) dot (sigma^2_i V) dot V^(-1/2) = sigma^2_i I
$

SPMでは$V$の推定は、ReML（制限付き最大尤度法）で行われる。GLMのパラメータ推定には、最小二乗法や最大尤度法が用いられる。推定された回帰係数 $beta$ は、各説明変数がBOLD信号に与える影響を示し、統計的検定を通じて有意性が評価される。

== 集団解析におけるランダム効果モデル <app6>
=== ランダム効果解析（Random Effects Analysis)
集団解析では、被験者間の変動を考慮するためにランダム効果モデル（Random Effects Model; RFX）を適用した。RFXモデルは、各被験者のデータを個別に解析した後、被験者間の変動を考慮して集団レベルでの統計的検定を行う手法である。被験者ごとの回帰係数を $beta_i$ とすると、RFXモデルは以下のように表される。
$
  bold(y)_i = bold(X)_i beta_i + bold(e)_i
$
$
  beta_i = beta_("all") + bold(u)_i
$
ここで、$bold(y)_i$ は被験者 $i$ のBOLD信号の時系列データ、$bold(X)_i$ は設計行列、$beta_i$ は被験者 $i$ の回帰係数、$bold(e)_i$ は誤差項、$beta_("all")$ は集団レベルの平均回帰係数、$bold(u)_i$ は被験者間のランダム効果を表す。

=== 要約統計量を使ったランダム効果解析
RFX解析では、各被験者の解析結果から要約統計量を抽出し、集団レベルでの解析に用いる
。具体的には、各被験者の回帰係数 $beta_i$ とその推定誤差の分散 $sigma^2_(beta_i)$ を計算する。これらの要約統計量を用いて、集団レベルでの統計的検定を行う。
母集団平均 $beta_("all")$ は、被験者ごとの要約統計量の平均として計算される
$
  hat(beta)_("all") = frac(1, N) sum_(i=1)^(N) macron(beta)_i
$
この推定値の分散$"Var"(beta_("all"))$ は、以下のように導出される。
$
  "Var"(hat(beta)_("all")) = frac(sigma^2_b, N) + frac(sigma^2_beta, N n)
$
この結果が、全データを用いた最大尤度（ML）推定による分散と完全に一致することが知られている @Penny2007。

== 多重比較補正 <app7>
=== ファミリー・ワイズ・エラー率（Family-Wise Error Rate; FWE）補正
fMRIデータの解析では、多数のボクセルに対して統計的検定を行うため、多重比較問題が生じる。これにより、偽陽性率（Type I error rate）が増加する可能性がある。ファミリー・ワイズ・エラー率（Family-Wise Error Rate; FWE）補正は、全体の誤検出率を制御するための手法であり、Bonferroni補正やランダムフィールド理論（Random Field Theory; RFT）に基づく方法などがある。
$
  "FWE" = P("全ての検定の中で少なくとも1つの偽陽性" | "帰無仮説が真")
$
すなわち、FWEは帰無仮説が真である場合に、全ての検定の中で少なくとも1つの偽陽性が発生する確率を表す。

==== ランダムフィールド理論（Random Field Theory; RFT）に基づくFWE補正
RFTに基づくFWE補正は、空間的に連続したfMRIデータの特性を考慮し、統計マップにおけるピークの分布をモデル化する手法である。RFTは、統計場がガウス過程として近似できると仮定し、ピークの高さやクラスタサイズに基づいて有意性を評価する @Worsley1996。これにより、ボクセルレベルおよびクラスターレベルでのFWE補正が可能となる。
$
  P("max Z" > u) approx sum_(d=0)^3 "R"_D(V) rho_D(t)
$
ここで、$u$ は観測された統計量の閾値、$"R"_D(V)$ はある領域$V$におけるレセル数、$rho_D(t)$ は期待エウラー標数と呼ばれる。その領域でtを偶然越えるレセルの期待値密度である。また、レセル数は以下のように計算される。
$
  "R"_D(V) = V / ( "FWHM"_x * "FWHM"_y * "FWHM"_z )
$
ここで、FWHMは半値全幅（Full Width at Half Maximum）を表し、空間の滑らかさの程度を示す。
FWE補正された$p$値は、以下のように計算される。
$
  p_("FWE") = P("max Z" > z_"obs")
$
ここで、$z_"obs"$ は観測された統計量の値である。

==== ボクセルレベルFWE補正
ボクセルレベルFWE補正では、各ボクセルに対して独立に統計的検定を行い、FWE補正された$p$値を計算する。RFTに基づく方法では、観測された統計量が閾値を超える確率を評価し、全体の誤検出率を制御する。

==== クラスターレベルFWE補正
クラスターレベルFWE補正では、連続したボクセルのクラスタに対して統計的検定を行い、クラスタサイズに基づいてFWE補正された$p$値を計算する。ボクセルレベルFWE補正では、ボクセル単位での検定を行うのに対し、クラスターレベルFWE補正では、空間的に連続したボクセル群（クラスタ）に対して検定を行う点が異なる。そのため、クラスターレベルFWE補正は、空間的に広がった効果を検出するのに適している。RFTに基づく方法では、クラスタの大きさが偶然に観測される確率を評価し、全体の誤検出率を制御する。

$
  p_("FWE, cluster") = P("max cluster size" > k_"obs")
$
ここで、$k_"obs"$ は観測されたクラスタサイズである。

==== Small Volume Correction (SVC)
Small Volume Correction (SVC)は、特定の関心領域（Region of Interest; ROI）に対してFWE補正を行う手法である。SVCでは、全脳解析に比べて検定の数が減少するため、より厳密なFWE補正が可能となる。SVCは、事前に定義されたROIに基づいて、統計マップ内のボクセルに対してFWE補正を適用する。これにより、特定の脳領域における効果の有意性を評価することができる。
$
  p_("SVC") = P("max Z" > z_"obs" | "within ROI")
$


== 表現類似性解析（RSA） <app8>

表現類似性解析（Representational Similarity Analysis; RSA）は、異なるシステム（例：脳とDNN）間の表現構造を比較するための手法である@Kriegeskorte2008。RSAでは、刺激セット内の各ペア間の類似度（または非類似度）を行列として表現し、この行列間の相関を計算することで表現構造の類似性を定量化する。

=== 表現非類似度行列（RDM）

表現非類似度行列（Representational Dissimilarity Matrix; RDM）は、$N$個の刺激に対する応答パターン間の非類似度を$N times N$の対称行列として表現したものである。RDMの各要素$(i, j)$は、刺激$i$と刺激$j$に対する応答パターン間の非類似度を示す。

脳活動の場合、各刺激に対するボクセルパターン$bold(b)_i in RR^V$（$V$はボクセル数）から、非類似度は以下のように計算される：
$
  "RDM"_(i,j) = 1 - "corr"(bold(b)_i, bold(b)_j)
$

ここで、$"corr"$はピアソン相関係数である。同様に、DNNの各層についても、刺激に対する活性化パターンからRDMを算出する。

=== 二重中心化（Double centering）

二重中心化は、RDMから行平均・列平均・全体平均を除去する前処理である。RDMを$bold(D)$とすると、二重中心化されたRDM $tilde(bold(D))$は以下のように計算される：
$
  tilde(D)_(i,j) = D_(i,j) - macron(D)_(i dot) - macron(D)_(dot j) + macron(D)_(dot dot)
$

ここで、$macron(D)_(i dot)$は行$i$の平均、$macron(D)_(dot j)$は列$j$の平均、$macron(D)_(dot dot)$は全体平均である。

二重中心化により、RDMの行和・列和がすべて0となり、絶対的なスケールではなく相対的なパターン構造のみを比較することが可能になる。この処理は、Centered Kernel Alignment（CKA）と等価であり、表現の類似性をより厳密に評価できる@Williams2024。

=== ノイズ上限値（Noise Ceiling）

ノイズ上限値は、被験者間の一致度から推定される理論的な説明力の上限である。Leave-One-Out法によるノイズ上限値は以下のように算出される：

- *NC上限*：各被験者のRDMと全被験者平均RDMとの相関の平均値
- *NC下限*：各被験者のRDMと当該被験者を除いた残りの被験者の平均RDMとの相関の平均値

NC上限比（モデル相関 / NC上限 × 100）により、理論的に達成可能な最大説明力に対するモデルの到達度を評価する。NC上限比が100%に近いほど、モデルが被験者間で共有される表現構造を完全に説明していることを示す。

== 3レベル階層的PCA分析 <app9>

DNNの各層の活性化パターンとfMRIデータを比較するため、3レベル階層的PCAを用いた手法を開発した。この手法は、DNNの活性化パターンを直交化された階層構造に分解し、各レベルの独立した説明力を評価可能とする。

=== 層グループの定義

DNNの層を情報処理の階層段階に基づいて4つのグループに分類した：

- *Initial層*（初期層）：低次視覚特徴（エッジ、テクスチャ）を抽出する初期層
- *Middle層*（中間層）：中次特徴を抽出する中間層
- *Late層*（後期層）：高次特徴（オブジェクトの形状やカテゴリ）を抽出する後期層
- *Final層*（最終層）：最終的な分類や埋め込み表現を生成する層

各層グループ $g in {1, 2, 3, 4}$ に対して、グループ内の全層の活性化ベクトルを連結した行列を $bold(H)_g in RR^(N times d_g)$ とする（$N$ は画像数、$d_g$ はグループ $g$ の総次元数）。



=== 3レベル階層的PCA

*レベル1：Global PC（全層共通成分）*

まず、全層グループの活性化を連結した行列 $bold(H)_("all") = [bold(H)_1, bold(H)_2, bold(H)_3, bold(H)_4]$ に対してPCAを適用し、全層に共通する分散を捕捉する主成分を抽出する。累積寄与率が60%に達するまでの主成分を選択し、これをGlobal PCとする。

$
  bold(Z)_("Global") = tilde(bold(H))_("all") bold(W)_("Global")
  quad in quad RR^(N times M_("Global"))
$

ここで、$bold(W)_("Global")$ はGlobal PCへの射影行列、$M_("Global")$ はGlobal PCの次元数である。

*レベル2：Layer-Shared PC（隣接層間共有成分）*

隣接する層グループ間で共有される分散を捕捉するため、正準相関分析（CCA）を用いる。まず、各層グループの活性化からGlobal成分を除去した残差を計算する：

$
  bold(R)_g = tilde(bold(H))_g - bold(Z)_("Global") bold(B)_g
$

ここで、$bold(B)_g$ は層グループ $g$ の活性化をGlobal PCで回帰した係数行列である。

次に、隣接する層グループペア $(g, g+1)$ に対してCCAを適用し、共有成分を抽出する：

$
  max_(bold(a), bold(b)) "corr"(bold(R)_g bold(a), bold(R)_(g+1) bold(b))
$

各隣接ペアについて上位2成分を抽出し、計6成分（3ペア × 2成分）をLayer-Shared PCとする。

*レベル3：Layer-Specific PC（層グループ固有成分）*

各層グループに固有の分散を捕捉するため、Global成分とShared成分を除去した残差に対してPCAを適用する：

$
  bold(R)'_g = bold(R)_g - sum_(j in N(g)) bold(Z)_("Shared",j) bold(C)_(g,j)
$

ここで、$N(g)$ は層グループ $g$ に関連するShared成分の集合、$bold(C)_(g,j)$ は対応する回帰係数である。残差 $bold(R)'_g$ に対してPCAを適用し、累積寄与率が50%に達するまでの主成分を各グループについて抽出する。

=== 直交化の意義

この3レベル階層的PCAにより、以下の直交構造が保証される：

1. Global PC、Layer-Shared PC、Layer-Specific PCは互いに直交
2. 各レベル内の成分も互いに直交
3. 異なる層グループのLayer-Specific PC同士も直交

この直交化により、GLMにおいて各レベル・各層グループの*独立した*説明力を評価可能となる。従来のPCAのみを用いた場合、層間で共有される分散と層固有の分散が混在し、各層の独自の寄与を分離できない問題があった。

=== GLMへの適用

これらの主成分を画像提示時のパラメトリックモジュレータとしてGLMに投入する。パラメトリックモジュレータは日ごとにまとめ（3セッション）、画像提示時の定数項はランごとに設定した。SPMの自動直交化は無効化し（orth = 0）、事前に直交化された主成分構造を保持する。

重要な点として、DNNの活性化から抽出した主成分スコアは*被験者全体で共通の変数*として使用した。これは、同一の食品画像に対するDNNの活性化パターンは被験者間で同一であり、各画像に対して一意の主成分スコアが割り当てられるためである。すなわち、各被験者の複数セッションにわたって提示された同一画像には同一の主成分スコアが適用される。

頭部運動パラメータ（6パラメータ）はランごとに共変量として含めた。統計的検定のためのコントラストはセッション間で平均化した。

統計検定では、各レベルおよび各層グループについてF検定を用いて説明力を評価する：

- *Global_F*：全てのGlobal PCの説明力
- *Shared_F*：全てのLayer-Shared PCの説明力
- *Initial_F, Middle_F, Late_F, Final_F*：各層グループ固有のLayer-Specific PCの説明力

有意な脳領域を同定するため、クラスターレベルFWE補正（クラスター形成閾値：p < 0.001 uncorrected、クラスターレベルFWE：p < 0.05）を適用する。

== 補足：事前学習済みConvNeXtの層別情報表現 <app10>

事前学習済み（ファインチューニングなし）のConvNeXt-Baseモデルにおける各層の情報表現を分析した結果を示す。本文中の@fig:repr1 ではファインチューニング後のConvNeXtモデルを示しているが、ここでは比較のため事前学習済みモデルの結果を示す。

#figure(
  image("./image/encoding_lineplot_convnext_36layers.png", width: 80%),
  supplement: "図",
  caption: [事前学習済みConvNeXt-Baseの各層における情報表現。各線は、各属性（主観的価値、健康度、栄養価、色）の予測と実際の評価との相関係数を示す。ファインチューニング後のモデル（@fig:repr1）と比較して、全体的に同様の傾向が見られるが、主観的価値の予測精度は後期層でも約0.55程度に留まる。],
) <fig:encoding_pretrained_convnext>

== 補足：DNN比較分析 ROI効果量 <app11>

効果量をRMS（β値の二乗平均平方根）で示す。エラーバーは被験者間の標準誤差（SEM）。

#figure(
  image("./image/roi_rms_barplot.png", width: 100%),
  supplement: "図",
  caption: [各層固有成分のROI効果量。\*は有意（SVC FWE補正 p < 0.05）。],
) <fig:roi_rms_barplot>

#figure(
  image("./image/roi_rms_barplot_all.png", width: 100%),
  supplement: "図",
  caption: [各層固有成分・共有成分・Global成分のROI効果量。共有(初-中)は初期-中間層間、共有(中-後)は中間-後期層間、共有(後-最)は後期-最終層間の共有成分。\*は有意（SVC FWE補正 p < 0.05）。],
) <fig:roi_rms_barplot_all>

#figure(
  image("./image/roi_rms_barplot_withshared_withglobal.png", width: 100%),
  supplement: "図",
  caption: [各層+共有+Global成分のROI効果量。各層固有成分に関連する共有成分およびGlobal成分を加えた効果量。\*は有意（SVC FWE補正 p < 0.05）。],
) <fig:roi_rms_barplot_withshared_withglobal>


